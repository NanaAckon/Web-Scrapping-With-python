{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd27f376-5786-46e9-ac9e-25966ee7ad97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching page 1: 403 Client Error: Forbidden for url: https://businessday.ng/tag/bdlead/page/1/?amp\n",
      "Error fetching page 2: 403 Client Error: Forbidden for url: https://businessday.ng/tag/bdlead/page/2/?amp\n",
      "Error fetching page 3: 403 Client Error: Forbidden for url: https://businessday.ng/tag/bdlead/page/3/?amp\n",
      "Scraping completed successfully!\n",
      "Collected 0 articles\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from random import randint\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "# Configure headers to mimic a real browser\n",
    "headers = {\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Configure retry strategy\n",
    "retry_strategy = Retry(\n",
    "    total=5,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "session = requests.Session()\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "# Lists to store data\n",
    "article_title = []\n",
    "article_date = []\n",
    "article_content = []\n",
    "article_url = []\n",
    "article_author = []\n",
    "\n",
    "pages = np.arange(1, 4)  # Adjust number of pages as needed\n",
    "\n",
    "for page_num in pages:\n",
    "    page_url = f\"https://businessday.ng/tag/bdlead/page/{page_num}/?amp\"\n",
    "    \n",
    "    try:\n",
    "        # Respectful delay with random interval\n",
    "        time.sleep(randint(3, 10))\n",
    "        \n",
    "        # Fetch the page with retries\n",
    "        response = session.get(page_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles = soup.find_all('div', class_='post-info')\n",
    "        \n",
    "        if not articles:\n",
    "            print(f\"No articles found on page {page_num}. Stopping.\")\n",
    "            break\n",
    "            \n",
    "        for article in articles:\n",
    "            # Extract Title and URL from listing page\n",
    "            title = article.find('h2', class_='post-title')\n",
    "            if title and title.find('a'):\n",
    "                article_title.append(title.text.strip())\n",
    "                url = title.find('a')['href']\n",
    "                article_url.append(url)\n",
    "            else:\n",
    "                # Skip articles without title/URL\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Visit individual article page\n",
    "                time.sleep(randint(1, 3))  # Add delay between article requests\n",
    "                article_res = session.get(url, headers=headers, timeout=10)\n",
    "                article_res.raise_for_status()\n",
    "                article_soup = BeautifulSoup(article_res.text, 'lxml')\n",
    "\n",
    "                # Extract content from article page\n",
    "                content_tag = article_soup.find('div', class_='post-content')\n",
    "                article_content.append(\n",
    "                    content_tag.get_text(strip=True) if content_tag \n",
    "                    else 'No content available'\n",
    "                )\n",
    "\n",
    "                # Extract author from article page\n",
    "                author_tag = article_soup.find('p', class_='author-name')\n",
    "                article_author.append(\n",
    "                    author_tag.text.strip() if author_tag \n",
    "                    else 'No Author'\n",
    "                )\n",
    "\n",
    "                # Extract date from article page\n",
    "                date_tag = article_soup.find('p', class_='post-date')\n",
    "                article_date.append(\n",
    "                    date_tag.text.strip() if date_tag \n",
    "                    else 'No Date'\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article {url}: {e}\")\n",
    "                # Append placeholders if article page fails\n",
    "                article_content.append('Content unavailable')\n",
    "                article_author.append('Author unavailable')\n",
    "                article_date.append('Date unavailable')\n",
    "                continue\n",
    "                \n",
    "        print(f\"Successfully processed page {page_num}\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching page {page_num}: {e}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"General error on page {page_num}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Create DataFrame\n",
    "articles_df = pd.DataFrame({\n",
    "    \"Title\": article_title,\n",
    "    \"Date\": article_date,\n",
    "    \"Content\": article_content,\n",
    "    \"URL\": article_url,\n",
    "    \"Author\": article_author\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "articles_df.to_csv(\"businessday00_articles00.csv\", index=False)\n",
    "print(\"Scraping completed successfully!\")\n",
    "print(f\"Collected {len(articles_df)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb790f7d-297a-4acd-a02a-fd824293086b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "      <th>URL</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exclusive: House bill to stop age restrictions...</td>\n",
       "      <td>April 11, 2025</td>\n",
       "      <td>…as banks risk suspension of operating licence...</td>\n",
       "      <td>https://businessday.ng/news/article/exclusive-...</td>\n",
       "      <td>Godsgift Onyedinefu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naira’s 3.6% fall against USD ‘reasonable’ com...</td>\n",
       "      <td>April 11, 2025</td>\n",
       "      <td>Naira’s steep fall against the US dollar over ...</td>\n",
       "      <td>https://businessday.ng/news/article/nairas-3-6...</td>\n",
       "      <td>Wasiu Alli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Financial constraints, last-minute changes del...</td>\n",
       "      <td>April 11, 2025</td>\n",
       "      <td>The Federal Government has completed the vetti...</td>\n",
       "      <td>https://businessday.ng/news/article/financial-...</td>\n",
       "      <td>Taofeek Oyedokun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naira blip temporary as analysts bet on rebound</td>\n",
       "      <td>April 11, 2025</td>\n",
       "      <td>The naira has weakened against the dollar due ...</td>\n",
       "      <td>https://businessday.ng/pro/article/naira-blip-...</td>\n",
       "      <td>Eniola Olatunji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Airlines slash fares to Europe on low travels</td>\n",
       "      <td>April 11, 2025</td>\n",
       "      <td>…As global carriers cut forecastsAs the aviati...</td>\n",
       "      <td>https://businessday.ng/aviation/article/airlin...</td>\n",
       "      <td>Ifeoma Okeke-Korieocha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title            Date  \\\n",
       "0  Exclusive: House bill to stop age restrictions...  April 11, 2025   \n",
       "1  Naira’s 3.6% fall against USD ‘reasonable’ com...  April 11, 2025   \n",
       "2  Financial constraints, last-minute changes del...  April 11, 2025   \n",
       "3    Naira blip temporary as analysts bet on rebound  April 11, 2025   \n",
       "4      Airlines slash fares to Europe on low travels  April 11, 2025   \n",
       "\n",
       "                                             Content  \\\n",
       "0  …as banks risk suspension of operating licence...   \n",
       "1  Naira’s steep fall against the US dollar over ...   \n",
       "2  The Federal Government has completed the vetti...   \n",
       "3  The naira has weakened against the dollar due ...   \n",
       "4  …As global carriers cut forecastsAs the aviati...   \n",
       "\n",
       "                                                 URL                  Author  \n",
       "0  https://businessday.ng/news/article/exclusive-...     Godsgift Onyedinefu  \n",
       "1  https://businessday.ng/news/article/nairas-3-6...              Wasiu Alli  \n",
       "2  https://businessday.ng/news/article/financial-...        Taofeek Oyedokun  \n",
       "3  https://businessday.ng/pro/article/naira-blip-...         Eniola Olatunji  \n",
       "4  https://businessday.ng/aviation/article/airlin...  Ifeoma Okeke-Korieocha  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86d51d4a-007b-4151-9c3f-6ecd79efa680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cloudscraperNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading cloudscraper-1.2.71-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in c:\\users\\user\\anacodaa\\lib\\site-packages (from cloudscraper) (3.1.2)\n",
      "Requirement already satisfied: requests>=2.9.2 in c:\\users\\user\\anacodaa\\lib\\site-packages (from cloudscraper) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt>=0.9.1 in c:\\users\\user\\anacodaa\\lib\\site-packages (from cloudscraper) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (2025.1.31)\n",
      "Downloading cloudscraper-1.2.71-py2.py3-none-any.whl (99 kB)\n",
      "Installing collected packages: cloudscraper\n",
      "Successfully installed cloudscraper-1.2.71\n"
     ]
    }
   ],
   "source": [
    "pip install cloudscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b745cb5a-c79e-4118-86c8-1172dde58d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anacodaa\\lib\\site-packages (2.2.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: cloudscraper in c:\\users\\user\\anacodaa\\lib\\site-packages (1.2.71)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\anacodaa\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in c:\\users\\user\\anacodaa\\lib\\site-packages (from cloudscraper) (3.1.2)\n",
      "Requirement already satisfied: requests>=2.9.2 in c:\\users\\user\\anacodaa\\lib\\site-packages (from cloudscraper) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt>=0.9.1 in c:\\users\\user\\anacodaa\\lib\\site-packages (from cloudscraper) (1.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\anacodaa\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anacodaa\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas cloudscraper beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abbc57af-ea29-4c79-afc2-761358807219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anacodaa\\lib\\site-packages (2.2.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anacodaa\\lib\\site-packages (2.32.3)\n",
      "Collecting requests-html\n",
      "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anacodaa\\lib\\site-packages (1.26.4)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Collecting pyquery (from requests-html)\n",
      "  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests-html) (2.1.0)\n",
      "Collecting parse (from requests-html)\n",
      "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: w3lib in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests-html) (2.1.2)\n",
      "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
      "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\anacodaa\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (1.4.4)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (7.0.1)\n",
      "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (4.66.5)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "Collecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading websockets-10.4.tar.gz (84 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anacodaa\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\anacodaa\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Requirement already satisfied: lxml>=2.1 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pyquery->requests-html) (5.2.1)\n",
      "Requirement already satisfied: cssselect>=1.2.0 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pyquery->requests-html) (1.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\anacodaa\\lib\\site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\anacodaa\\lib\\site-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anacodaa\\lib\\site-packages (from tqdm<5.0.0,>=4.42.1->pyppeteer>=0.0.14->requests-html) (0.4.6)\n",
      "Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
      "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "Downloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
      "Downloading pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
      "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
      "Building wheels for collected packages: websockets\n",
      "  Building wheel for websockets (setup.py): started\n",
      "  Building wheel for websockets (setup.py): finished with status 'done'\n",
      "  Created wheel for websockets: filename=websockets-10.4-cp312-cp312-win_amd64.whl size=95034 sha256=6c0c9d6dd22aff6006fe6847a8c3ba810ef056fdb24dac976897861803390b66\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\80\\cf\\6d\\5d7e4c920cb41925a178b2d2621889c520d648bab487b1d7fd\n",
      "Successfully built websockets\n",
      "Installing collected packages: parse, websockets, urllib3, pyquery, pyee, pyppeteer, bs4, requests-html\n",
      "  Attempting uninstall: websockets\n",
      "    Found existing installation: websockets 15.0.1\n",
      "    Uninstalling websockets-15.0.1:\n",
      "      Successfully uninstalled websockets-15.0.1\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "Successfully installed bs4-0.0.2 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 urllib3-1.26.20 websockets-10.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.114 requires urllib3>=2.2.2, but you have urllib3 1.26.20 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas requests requests-html numpy bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c69a6a35-0f80-4bc6-bca4-8656c5faa369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml-html-clean==0.1.1 in c:\\users\\user\\anacodaa\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\user\\anacodaa\\lib\\site-packages (from lxml-html-clean==0.1.1) (5.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml-html-clean==0.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e55189b0-3af5-412c-ad7b-3cd11a8bf3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests-html in c:\\users\\user\\anacodaa\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests-html) (2.32.3)\n",
      "Requirement already satisfied: pyquery in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests-html) (2.0.1)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests-html) (2.1.0)\n",
      "Requirement already satisfied: parse in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests-html) (1.20.2)\n",
      "Requirement already satisfied: bs4 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests-html) (0.0.2)\n",
      "Requirement already satisfied: w3lib in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests-html) (2.1.2)\n",
      "Requirement already satisfied: pyppeteer>=0.0.14 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests-html) (2.0.0)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (1.4.4)\n",
      "Requirement already satisfied: certifi>=2023 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (2025.1.31)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (7.0.1)\n",
      "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (11.1.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (4.66.5)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (1.26.20)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (10.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\anacodaa\\lib\\site-packages (from bs4->requests-html) (4.12.3)\n",
      "Requirement already satisfied: lxml>=2.1 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pyquery->requests-html) (5.2.1)\n",
      "Requirement already satisfied: cssselect>=1.2.0 in c:\\users\\user\\anacodaa\\lib\\site-packages (from pyquery->requests-html) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests->requests-html) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests->requests-html) (3.7)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\anacodaa\\lib\\site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\anacodaa\\lib\\site-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anacodaa\\lib\\site-packages (from tqdm<5.0.0,>=4.42.1->pyppeteer>=0.0.14->requests-html) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\anacodaa\\lib\\site-packages (from beautifulsoup4->bs4->requests-html) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests-html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89afbc90-d427-4d65-a73a-b392502b0ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cloudscraper in c:\\users\\user\\anacodaa\\lib\\site-packages (1.2.71)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in c:\\users\\user\\anacodaa\\lib\\site-packages (from cloudscraper) (3.1.2)\n",
      "Requirement already satisfied: requests>=2.9.2 in c:\\users\\user\\anacodaa\\lib\\site-packages (from cloudscraper) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt>=0.9.1 in c:\\users\\user\\anacodaa\\lib\\site-packages (from cloudscraper) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anacodaa\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "pip install cloudscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02359d40-d0e0-4e65-80f9-b5c6f49b558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudscraper\n",
    "from bs4 import BeauttifulSoup\n",
    "scraper = cloudscraper.create_scraper() \n",
    "\n",
    "soup = Beautifulsoup(scraper.get(\"https://businessday.ng/tag/bdlead/page/{page_num}/?amp\").text, 'html parser')\n",
    "\n",
    "print(soup. text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfe6fc2f-a5df-4f2c-9b05-9dd5cec53a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching/processing page 1: 403 Client Error: Forbidden for url: https://businessday.ng/tag/bdlead/page/1/?amp\n",
      "Error fetching/processing page 2: 403 Client Error: Forbidden for url: https://businessday.ng/tag/bdlead/page/2/?amp\n",
      "Error fetching/processing page 3: 403 Client Error: Forbidden for url: https://businessday.ng/tag/bdlead/page/3/?amp\n",
      "Scraping completed successfully!\n",
      "Collected 0 articles\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from random import randint\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "# Configure headers to mimic a real browser\n",
    "headers = {\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Configure retry strategy\n",
    "retry_strategy = Retry(\n",
    "    total=8,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "\n",
    "# Create cloudscraper instance with retry capabilities\n",
    "scraper = cloudscraper.create_scraper()\n",
    "scraper.mount(\"https://\", adapter)\n",
    "scraper.mount(\"http://\", adapter)\n",
    "\n",
    "# Lists to store data\n",
    "article_title = []\n",
    "article_date = []\n",
    "article_content = []\n",
    "article_url = []\n",
    "article_author = []\n",
    "\n",
    "pages = np.arange(1, 4)  # Adjust number of pages as needed\n",
    "\n",
    "for page_num in pages:\n",
    "    page_url = f\"https://businessday.ng/tag/bdlead/page/{page_num}/?amp\"\n",
    "    \n",
    "    try:\n",
    "        # Respectful delay with random interval\n",
    "        time.sleep(randint(3, 10))\n",
    "        \n",
    "        # Fetch the page with retries and cloudscraper\n",
    "        response = scraper.get(page_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles = soup.find_all('div', class_='post-info')\n",
    "        \n",
    "        if not articles:\n",
    "            print(f\"No articles found on page {page_num}. Stopping.\")\n",
    "            break\n",
    "            \n",
    "        for article in articles:\n",
    "            # Extract Title and URL from listing page\n",
    "            title = article.find('h2', class_='post-title')\n",
    "            if title and title.find('a'):\n",
    "                article_title.append(title.text.strip())\n",
    "                url = title.find('a')['href']\n",
    "                article_url.append(url)\n",
    "            else:\n",
    "                continue  # Skip articles without title/URL\n",
    "                \n",
    "            try:\n",
    "                # Visit individual article page\n",
    "                time.sleep(randint(1, 3))\n",
    "                article_res = scraper.get(url, headers=headers, timeout=10)\n",
    "                article_res.raise_for_status()\n",
    "                article_soup = BeautifulSoup(article_res.text, 'lxml')\n",
    "\n",
    "                # Extract content from article page\n",
    "                content_tag = article_soup.find('div', class_='post-content')\n",
    "                article_content.append(\n",
    "                    content_tag.get_text(strip=True) if content_tag \n",
    "                    else 'No content available'\n",
    "                )\n",
    "\n",
    "                # Extract author from article page\n",
    "                author_tag = article_soup.find('p', class_='author-name')\n",
    "                article_author.append(\n",
    "                    author_tag.text.strip() if author_tag \n",
    "                    else 'No Author'\n",
    "                )\n",
    "\n",
    "                # Extract date from article page\n",
    "                date_tag = article_soup.find('p', class_='post-date')\n",
    "                article_date.append(\n",
    "                    date_tag.text.strip() if date_tag \n",
    "                    else 'No Date'\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article {url}: {e}\")\n",
    "                # Append placeholders if article page fails\n",
    "                article_content.append('Content unavailable')\n",
    "                article_author.append('Author unavailable')\n",
    "                article_date.append('Date unavailable')\n",
    "                continue\n",
    "                \n",
    "        print(f\"Successfully processed page {page_num}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching/processing page {page_num}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Create DataFrame\n",
    "articles_df = pd.DataFrame({\n",
    "    \"Title\": article_title,\n",
    "    \"Date\": article_date,\n",
    "    \"Content\": article_content,\n",
    "    \"URL\": article_url,\n",
    "    \"Author\": article_author\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "articles_df.to_csv(\"businessday_articles111.csv\", index=False)\n",
    "print(\"Scraping completed successfully!\")\n",
    "print(f\"Collected {len(articles_df)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31670a-9824-4e7f-a49f-c9f9a7579623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
