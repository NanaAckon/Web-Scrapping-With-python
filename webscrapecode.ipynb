{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20f962b2-0fc0-4e8a-a105-76ed6ff5a00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fresh scrape...\n",
      "Navigating to https://businessday.ng/tag/bdlead/?amp\n",
      "Initial page loaded\n",
      "\n",
      "Processing page 1\n",
      "Current page title: BDlead Archives - Businessday NG\n",
      "Found 10 news items\n",
      "Added article: Oil palm growers to replant 1.5m hectares on risin...\n",
      "Added article: How CBEX wiped off investors’ N1.3trn in nine mont...\n",
      "Added article: Metering hits 4-year low as FG misses target...\n",
      "Added article: Investment Act opens window for digital asset mark...\n",
      "Added article: Alake, Ayeni others to headline BusinessDay solid ...\n",
      "Added article: Oyo, Kaduna, Kebbi record highest food inflation i...\n",
      "Added article: Why March inflation defied  analyst projections, i...\n",
      "Added article: DISCLAIMER: False attribution of article to Busine...\n",
      "Added article: Access Bank acquires National Bank of Kenya to boo...\n",
      "Added article: Full list of 51 people killed in Plateau Monday mo...\n",
      "Successfully scraped page 1 with 10 new articles\n",
      "Downloading content for article: Oil palm growers to replant 1.5m hectares on risin...\n",
      "Extracted content for: Oil palm growers to replant 1.5m hectares on risin...\n",
      "An error occurred: Message: invalid session id\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00C28073+60707]\n",
      "\tGetHandleVerifier [0x00C280B4+60772]\n",
      "\t(No symbol) [0x00A504FE]\n",
      "\t(No symbol) [0x00A8B898]\n",
      "\t(No symbol) [0x00ABCF06]\n",
      "\t(No symbol) [0x00AB89D5]\n",
      "\t(No symbol) [0x00AB7F66]\n",
      "\t(No symbol) [0x00A236E5]\n",
      "\t(No symbol) [0x00A23C3E]\n",
      "\t(No symbol) [0x00A240CD]\n",
      "\tGetHandleVerifier [0x00E6BB53+2435075]\n",
      "\tGetHandleVerifier [0x00E670F3+2416035]\n",
      "\tGetHandleVerifier [0x00E8349C+2531660]\n",
      "\tGetHandleVerifier [0x00C3F145+155125]\n",
      "\tGetHandleVerifier [0x00C45AED+182173]\n",
      "\t(No symbol) [0x00A233B0]\n",
      "\t(No symbol) [0x00A22BC3]\n",
      "\tGetHandleVerifier [0x00F8D23C+3620588]\n",
      "\tBaseThreadInitThunk [0x76025D49+25]\n",
      "\tRtlInitializeExceptionChain [0x777ACF0B+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x777ACE91+561]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Session directory to maintain cookies between runs\n",
    "SESSION_DIR = os.path.join(os.getcwd(), \"chrome_session\")\n",
    "os.makedirs(SESSION_DIR, exist_ok=True)\n",
    "\n",
    "# File to store checkpoint data\n",
    "CHECKPOINT_FILE = \"scraper_checkpoint.json\"\n",
    "\n",
    "def remove_existing_files():\n",
    "    \"\"\"Remove existing checkpoint and data files to ensure a fresh start\"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        os.remove(CHECKPOINT_FILE)\n",
    "    if os.path.exists('businessday_progress3.csv'):\n",
    "        os.remove('businessday_progress3.csv')\n",
    "    if os.path.exists('businessday_final3.csv'):\n",
    "        os.remove('businessday_final3.csv')\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Set up the undetected Chrome driver with necessary options\"\"\"\n",
    "    options = uc.ChromeOptions()\n",
    "    # Add user data directory to maintain session/cookies\n",
    "    options.add_argument(f\"--user-data-dir={SESSION_DIR}\")\n",
    "    # Make browser less detectable\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-infobars\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    # Use a realistic user agent\n",
    "    options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36\")\n",
    "    # Initialize undetected-chromedriver which helps bypass Cloudflare\n",
    "    driver = uc.Chrome(options=options)\n",
    "    # Set page load timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def random_sleep(min_seconds=2, max_seconds=5):\n",
    "    \"\"\"Sleep for a random amount of time to appear more human-like\"\"\"\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "def human_like_scroll(driver):\n",
    "    \"\"\"Scroll down the page in a human-like manner\"\"\"\n",
    "    total_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    current_position = 0\n",
    "    while current_position < total_height:\n",
    "        scroll_increment = random.randint(100, 800)\n",
    "        current_position += scroll_increment\n",
    "        if current_position > total_height:\n",
    "            current_position = total_height\n",
    "        driver.execute_script(f\"window.scrollTo(0, {current_position});\")\n",
    "        random_sleep(0.5, 1.5)\n",
    "\n",
    "def scrape_page(driver, scraped_data):\n",
    "    \"\"\"\n",
    "    Scrape the current page for articles.\n",
    "    Each article is stored as a dictionary with content initially set to None.\n",
    "    Returns a list of new articles scraped from this page.\n",
    "    \"\"\"\n",
    "    if \"Just a moment\" in driver.title:\n",
    "        print(\"Still on Cloudflare challenge page. Waiting...\")\n",
    "        time.sleep(15)\n",
    "        return []\n",
    "\n",
    "    print(f\"Current page title: {driver.title}\")\n",
    "    human_like_scroll(driver)\n",
    "    new_articles = []\n",
    "    try:\n",
    "        # Wait for the news container to be present\n",
    "        news_container = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.news\"))\n",
    "        )\n",
    "        # Find all news items\n",
    "        news_items = news_container.find_elements(By.XPATH, './/div[@class=\"post-info\"]')\n",
    "        print(f\"Found {len(news_items)} news items\")\n",
    "        if not news_items:\n",
    "            print(\"No news items found on the page.\")\n",
    "            return new_articles\n",
    "\n",
    "        for item in news_items:\n",
    "            try:\n",
    "                title_element = item.find_element(By.TAG_NAME, 'h2').find_element(By.TAG_NAME, 'a')\n",
    "                title = title_element.text\n",
    "                url = title_element.get_attribute(\"href\")\n",
    "                author = item.find_element(By.CLASS_NAME, 'post-author').find_element(By.TAG_NAME, 'a').text\n",
    "                date = item.find_element(By.CLASS_NAME, 'post-date').text\n",
    "                excerpt = item.find_element(By.TAG_NAME, 'p').text\n",
    "\n",
    "                # Avoid duplicates by checking if an article with the same title already exists\n",
    "                if not any(title == article['Title'] for article in scraped_data):\n",
    "                    article_dict = {\n",
    "                        'Title': title,\n",
    "                        'Author': author,\n",
    "                        'Date': date,\n",
    "                        'Excerpt': excerpt,\n",
    "                        'URL': url,\n",
    "                        'Content': None  # Content will be filled in immediately after scraping the page\n",
    "                    }\n",
    "                    scraped_data.append(article_dict)\n",
    "                    new_articles.append(article_dict)\n",
    "                    print(f\"Added article: {title[:50]}...\")\n",
    "                else:\n",
    "                    print(f\"Skipping duplicate article: {title[:50]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing news item: {e}\")\n",
    "                continue\n",
    "        return new_articles\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping page: {e}\")\n",
    "        return new_articles\n",
    "\n",
    "def extract_article_content(driver, article):\n",
    "    \"\"\"\n",
    "    Opens article URL in a new tab, extracts its content, and updates the article dictionary.\n",
    "    \"\"\"\n",
    "    original_window = driver.current_window_handle\n",
    "    try:\n",
    "        # Open a new tab\n",
    "        driver.execute_script(\"window.open('');\")\n",
    "        # Switch to new tab\n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "        driver.get(article['URL'])\n",
    "        # Allow the page to load\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            # Wait until the post content is present\n",
    "            content_element = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, 'post-content'))\n",
    "            )\n",
    "            content = content_element.text\n",
    "            article['Content'] = content.strip()\n",
    "            print(f\"Extracted content for: {article['Title'][:50]}...\")\n",
    "        except Exception as e:\n",
    "            article['Content'] = 'Content not found'\n",
    "            print(f\"Content extraction failed for: {article['Title'][:50]}: {e}\")\n",
    "    except Exception as e:\n",
    "        article['Content'] = 'Content extraction error'\n",
    "        print(f\"Error processing article URL {article['URL']}: {e}\")\n",
    "    finally:\n",
    "        # Close the tab and switch back to the original window\n",
    "        driver.close()\n",
    "        driver.switch_to.window(original_window)\n",
    "\n",
    "def main():\n",
    "    print(\"Starting fresh scrape...\")\n",
    "    # Remove existing files to ensure a fresh start\n",
    "    remove_existing_files()\n",
    "\n",
    "    # Initialize fresh scraping variables\n",
    "    page_count = 1\n",
    "    start_url = 'https://businessday.ng/tag/bdlead/?amp'\n",
    "    scraped_data = []\n",
    "\n",
    "    # Setup the driver\n",
    "    driver = setup_driver()\n",
    "\n",
    "    try:\n",
    "        # Navigate to the starting page\n",
    "        print(f\"Navigating to {start_url}\")\n",
    "        driver.get(start_url)\n",
    "        print(\"Initial page loaded\")\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Set the number of pages you want to process\n",
    "        max_pages = 3\n",
    "        while page_count <= max_pages:\n",
    "            print(f\"\\nProcessing page {page_count}\")\n",
    "            current_url = driver.current_url\n",
    "\n",
    "            # Scrape the page and get new articles\n",
    "            new_articles = scrape_page(driver, scraped_data)\n",
    "            if new_articles:\n",
    "                print(f\"Successfully scraped page {page_count} with {len(new_articles)} new articles\")\n",
    "            else:\n",
    "                print(f\"Failed to scrape page {page_count} or found no new articles\")\n",
    "\n",
    "            # For each new article scraped on this page, immediately extract the full content\n",
    "            for article in new_articles:\n",
    "                print(f\"Downloading content for article: {article['Title'][:50]}...\")\n",
    "                extract_article_content(driver, article)\n",
    "\n",
    "            # Save intermediate progress after each page\n",
    "            pd.DataFrame(scraped_data).to_csv('businessday_progress30.csv', index=False)\n",
    "\n",
    "            # Break if we have reached our target\n",
    "            if page_count >= max_pages:\n",
    "                break\n",
    "\n",
    "            # Navigate to the next page\n",
    "            try:\n",
    "                print(\"Looking for next page button...\")\n",
    "                next_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//a[@class=\"next page-numbers\"]'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_button)\n",
    "                random_sleep(1, 2)\n",
    "                print(\"Clicking next page button...\")\n",
    "                driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                print(\"Waiting for next page to load...\")\n",
    "                random_sleep(8, 12)\n",
    "                page_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error navigating to next page: {e}\")\n",
    "                print(\"Trying alternative method...\")\n",
    "                try:\n",
    "                    if \"page/\" in current_url:\n",
    "                        parts = current_url.split(\"page/\")\n",
    "                        current_page_num = int(parts[1].split(\"/\")[0])\n",
    "                        next_page_num = current_page_num + 1\n",
    "                        next_url = f\"{parts[0]}page/{next_page_num}/\"\n",
    "                    else:\n",
    "                        next_url = f\"{current_url}page/2/\"\n",
    "                    print(f\"Navigating directly to: {next_url}\")\n",
    "                    driver.get(next_url)\n",
    "                    random_sleep(8, 12)\n",
    "                    page_count += 1\n",
    "                except Exception as nav_error:\n",
    "                    print(f\"Failed to navigate to next page: {nav_error}\")\n",
    "                    break\n",
    "\n",
    "        # Save the final results\n",
    "        if scraped_data:\n",
    "            df = pd.DataFrame(scraped_data, columns=['Title', 'Author', 'Date', 'Excerpt', 'URL', 'Content'])\n",
    "            df.to_csv('businessday_final30.csv', index=False)\n",
    "            print(f\"\\nScraping completed. Saved {len(scraped_data)} articles\")\n",
    "        else:\n",
    "            print(\"No data was scraped\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nScraping interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e169574-bc7f-444d-90a4-11170e39ffa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Page 1\n",
      "Extracting content for: Oil palm growers to replant 1.5m hectares on risin...\n",
      "Main execution error: list index out of range\n",
      "\n",
      "Scraping completed. Final data saved.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Session directory to maintain cookies between runs\n",
    "SESSION_DIR = os.path.join(os.getcwd(), \"chrome_session\")\n",
    "os.makedirs(SESSION_DIR, exist_ok=True)\n",
    "\n",
    "def remove_existing_files():\n",
    "    \"\"\"Remove existing checkpoint and data files to ensure fresh start\"\"\"\n",
    "    files_to_remove = ['businessday_progress3.csv', 'businessday_final3.csv']\n",
    "    for file in files_to_remove:\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file)\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Set up the undetected Chrome driver with necessary options\"\"\"\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(f\"--user-data-dir={SESSION_DIR}\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36\")\n",
    "    driver = uc.Chrome(options=options)\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def random_sleep(min_seconds=1, max_seconds=3):\n",
    "    \"\"\"Human-like random delay\"\"\"\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "def human_like_scroll(driver):\n",
    "    \"\"\"Realistic scrolling behavior\"\"\"\n",
    "    scroll_pauses = [random.randint(100, 300) for _ in range(random.randint(3, 6))]\n",
    "    for pause in scroll_pauses:\n",
    "        driver.execute_script(f\"window.scrollBy(0, {pause});\")\n",
    "        random_sleep(0.2, 0.5)\n",
    "\n",
    "def scrape_page(driver, scraped_data):\n",
    "    \"\"\"Extract articles from current page\"\"\"\n",
    "    try:\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.news\"))\n",
    "        )\n",
    "        human_like_scroll(driver)\n",
    "        \n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, 'div.post-info')\n",
    "        new_articles = []\n",
    "        \n",
    "        for article in articles:\n",
    "            try:\n",
    "                title_elem = article.find_element(By.CSS_SELECTOR, 'h2 a')\n",
    "                title = title_elem.text\n",
    "                if any(a['Title'] == title for a in scraped_data):\n",
    "                    continue\n",
    "                \n",
    "                url = title_elem.get_attribute('href')\n",
    "                author = article.find_element(By.CSS_SELECTOR, '.post-author a').text\n",
    "                date = article.find_element(By.CSS_SELECTOR, '.post-date').text\n",
    "                excerpt = article.find_element(By.CSS_SELECTOR, 'p').text\n",
    "                \n",
    "                new_articles.append({\n",
    "                    'Title': title,\n",
    "                    'Author': author,\n",
    "                    'Date': date,\n",
    "                    'Excerpt': excerpt,\n",
    "                    'URL': url,\n",
    "                    'Content': None\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article preview: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return new_articles\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Page scraping error: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def extract_article_content(driver, url):\n",
    "    \"\"\"Extract full content from article page\"\"\"\n",
    "    main_window = driver.current_window_handle\n",
    "    driver.execute_script(\"window.open('');\")\n",
    "    driver.switch_to.window(driver.window_handles[1])\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, '.post-content'))\n",
    "        )\n",
    "        content = driver.find_element(By.CSS_SELECTOR, '.post-content').text\n",
    "        random_sleep(1, 2)  # Mimic reading time\n",
    "    except Exception as e:\n",
    "        print(f\"Content extraction error: {str(e)}\")\n",
    "        content = \"Content not available\"\n",
    "    finally:\n",
    "        driver.close()\n",
    "        driver.switch_to.window(main_window)\n",
    "    \n",
    "    return content.strip()\n",
    "\n",
    "def main():\n",
    "    remove_existing_files()\n",
    "    driver = setup_driver()\n",
    "    scraped_data = []\n",
    "    max_pages = 3\n",
    "    current_page = 1\n",
    "\n",
    "    try:\n",
    "        driver.get('https://businessday.ng/tag/bdlead/?amp')\n",
    "        random_sleep(2, 4)\n",
    "        \n",
    "        while current_page <= max_pages:\n",
    "            print(f\"\\nProcessing Page {current_page}\")\n",
    "            \n",
    "            # Scrape articles from current page\n",
    "            new_articles = scrape_page(driver, scraped_data)\n",
    "            \n",
    "            if not new_articles:\n",
    "                print(\"No new articles found, stopping...\")\n",
    "                break\n",
    "            \n",
    "            # Process each new article immediately\n",
    "            for article in new_articles:\n",
    "                print(f\"Extracting content for: {article['Title'][:50]}...\")\n",
    "                article['Content'] = extract_article_content(driver, article['URL'])\n",
    "                scraped_data.append(article)\n",
    "                \n",
    "                # Save progress after each article\n",
    "                pd.DataFrame(scraped_data).to_csv('businessday_progress30.csv', index=False)\n",
    "                random_sleep(1, 3)  # Between articles\n",
    "            \n",
    "            # Save final state for the page\n",
    "            pd.DataFrame(scraped_data).to_csv('businessday_final30.csv', index=False)\n",
    "            \n",
    "            # Navigate to next page\n",
    "            try:\n",
    "                next_btn = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, 'a.next.page-numbers'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", next_btn)\n",
    "                random_sleep(1, 2)\n",
    "                next_btn.click()\n",
    "                current_page += 1\n",
    "                random_sleep(3, 5)  # Wait for next page load\n",
    "            except Exception as e:\n",
    "                print(f\"Pagination error: {str(e)}\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Main execution error: {str(e)}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"\\nScraping completed. Final data saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f0a4b1a-3d78-416d-92ac-ea40151c8d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fresh scrape...\n",
      "Navigating to https://businessday.ng/tag/bdlead/?amp\n",
      "Initial page loaded\n",
      "\n",
      "Processing page 1\n",
      "Current page title: BDlead Archives - Businessday NG\n",
      "Found 10 news items\n",
      "Added article: Oil palm growers to replant 1.5m hectares on risin...\n",
      "Added article: How CBEX wiped off investors’ N1.3trn in nine mont...\n",
      "Added article: Metering hits 4-year low as FG misses target...\n",
      "Added article: Investment Act opens window for digital asset mark...\n",
      "Added article: Alake, Ayeni others to headline BusinessDay solid ...\n",
      "Added article: Oyo, Kaduna, Kebbi record highest food inflation i...\n",
      "Added article: Why March inflation defied  analyst projections, i...\n",
      "Added article: DISCLAIMER: False attribution of article to Busine...\n",
      "Added article: Access Bank acquires National Bank of Kenya to boo...\n",
      "Added article: Full list of 51 people killed in Plateau Monday mo...\n",
      "Successfully scraped page 1 with 10 new articles\n",
      "Downloading content for article: Oil palm growers to replant 1.5m hectares on risin...\n",
      "Extracted content for: Oil palm growers to replant 1.5m hectares on risin...\n",
      "Downloading content for article: How CBEX wiped off investors’ N1.3trn in nine mont...\n",
      "Extracted content for: How CBEX wiped off investors’ N1.3trn in nine mont...\n",
      "Downloading content for article: Metering hits 4-year low as FG misses target...\n",
      "Extracted content for: Metering hits 4-year low as FG misses target...\n",
      "Downloading content for article: Investment Act opens window for digital asset mark...\n",
      "Extracted content for: Investment Act opens window for digital asset mark...\n",
      "Downloading content for article: Alake, Ayeni others to headline BusinessDay solid ...\n",
      "Extracted content for: Alake, Ayeni others to headline BusinessDay solid ...\n",
      "Downloading content for article: Oyo, Kaduna, Kebbi record highest food inflation i...\n",
      "Extracted content for: Oyo, Kaduna, Kebbi record highest food inflation i...\n",
      "Downloading content for article: Why March inflation defied  analyst projections, i...\n",
      "Extracted content for: Why March inflation defied  analyst projections, i...\n",
      "Downloading content for article: DISCLAIMER: False attribution of article to Busine...\n",
      "Extracted content for: DISCLAIMER: False attribution of article to Busine...\n",
      "Downloading content for article: Access Bank acquires National Bank of Kenya to boo...\n",
      "Extracted content for: Access Bank acquires National Bank of Kenya to boo...\n",
      "Downloading content for article: Full list of 51 people killed in Plateau Monday mo...\n",
      "Extracted content for: Full list of 51 people killed in Plateau Monday mo...\n",
      "Looking for next page button...\n",
      "Clicking next page button...\n",
      "Waiting for next page to load...\n",
      "\n",
      "Processing page 2\n",
      "Current page title: BDlead Archives - Page 2 of 1641 - Businessday NG\n",
      "Found 10 news items\n",
      "Added article: PoS wars: Banks battle fintechs for market share...\n",
      "Added article: Declining oil output to hit Nigeria's dollar earni...\n",
      "Added article: How banks can accelerate Nigeria's $1trn GDP targe...\n",
      "Added article: GenCos threaten shutdown over N4trn debt...\n",
      "Added article: Forty-seven killed in Sunday night attack on Plate...\n",
      "Added article: Naira fall temporary as analysts bet on rebound...\n",
      "Added article: Nigeria missing in $224bn coffee market on aging t...\n",
      "Added article: Sesame holds billions in export upside as demand s...\n",
      "Added article: Fintechs outpace telcos in mobile money race...\n",
      "Added article: ‘Nothing new to reveal’, Presidency dismisses US c...\n",
      "Successfully scraped page 2 with 10 new articles\n",
      "Downloading content for article: PoS wars: Banks battle fintechs for market share...\n",
      "Extracted content for: PoS wars: Banks battle fintechs for market share...\n",
      "Downloading content for article: Declining oil output to hit Nigeria's dollar earni...\n",
      "Extracted content for: Declining oil output to hit Nigeria's dollar earni...\n",
      "Downloading content for article: How banks can accelerate Nigeria's $1trn GDP targe...\n",
      "Extracted content for: How banks can accelerate Nigeria's $1trn GDP targe...\n",
      "Downloading content for article: GenCos threaten shutdown over N4trn debt...\n",
      "Extracted content for: GenCos threaten shutdown over N4trn debt...\n",
      "Downloading content for article: Forty-seven killed in Sunday night attack on Plate...\n",
      "Extracted content for: Forty-seven killed in Sunday night attack on Plate...\n",
      "Downloading content for article: Naira fall temporary as analysts bet on rebound...\n",
      "Extracted content for: Naira fall temporary as analysts bet on rebound...\n",
      "Downloading content for article: Nigeria missing in $224bn coffee market on aging t...\n",
      "Extracted content for: Nigeria missing in $224bn coffee market on aging t...\n",
      "Downloading content for article: Sesame holds billions in export upside as demand s...\n",
      "Extracted content for: Sesame holds billions in export upside as demand s...\n",
      "Downloading content for article: Fintechs outpace telcos in mobile money race...\n",
      "Extracted content for: Fintechs outpace telcos in mobile money race...\n",
      "Downloading content for article: ‘Nothing new to reveal’, Presidency dismisses US c...\n",
      "Extracted content for: ‘Nothing new to reveal’, Presidency dismisses US c...\n",
      "Looking for next page button...\n",
      "Clicking next page button...\n",
      "Waiting for next page to load...\n",
      "\n",
      "Processing page 3\n",
      "Current page title: BDlead Archives - Page 3 of 1641 - Businessday NG\n",
      "Found 10 news items\n",
      "Added article: Kano Government defends blasphemy laws, rejects EC...\n",
      "Added article: US court orders FBI, DEA to release records on Tin...\n",
      "Added article: B/Haram: EU to spend ₦2.bn to revamp education, vo...\n",
      "Added article: Breath of fresh air for customers on estimated bil...\n",
      "Added article: 2027: Nigerian opposition yet to show commitment t...\n",
      "Added article: Tinubu's men in defensive mode as allegations of l...\n",
      "Added article: Interior minister rolls out sweeping immigration r...\n",
      "Added article: Ganduje leads APC's NWC to Buhari, says move to fo...\n",
      "Added article: US tightens visa interview rules for Nigerian appl...\n",
      "Added article: Former Super Eagles coach Christian Chukwu dies at...\n",
      "Successfully scraped page 3 with 10 new articles\n",
      "Downloading content for article: Kano Government defends blasphemy laws, rejects EC...\n",
      "Extracted content for: Kano Government defends blasphemy laws, rejects EC...\n",
      "Downloading content for article: US court orders FBI, DEA to release records on Tin...\n",
      "Extracted content for: US court orders FBI, DEA to release records on Tin...\n",
      "Downloading content for article: B/Haram: EU to spend ₦2.bn to revamp education, vo...\n",
      "Extracted content for: B/Haram: EU to spend ₦2.bn to revamp education, vo...\n",
      "Downloading content for article: Breath of fresh air for customers on estimated bil...\n",
      "Extracted content for: Breath of fresh air for customers on estimated bil...\n",
      "Downloading content for article: 2027: Nigerian opposition yet to show commitment t...\n",
      "Extracted content for: 2027: Nigerian opposition yet to show commitment t...\n",
      "Downloading content for article: Tinubu's men in defensive mode as allegations of l...\n",
      "Extracted content for: Tinubu's men in defensive mode as allegations of l...\n",
      "Downloading content for article: Interior minister rolls out sweeping immigration r...\n",
      "Extracted content for: Interior minister rolls out sweeping immigration r...\n",
      "Downloading content for article: Ganduje leads APC's NWC to Buhari, says move to fo...\n",
      "Extracted content for: Ganduje leads APC's NWC to Buhari, says move to fo...\n",
      "Downloading content for article: US tightens visa interview rules for Nigerian appl...\n",
      "Extracted content for: US tightens visa interview rules for Nigerian appl...\n",
      "Downloading content for article: Former Super Eagles coach Christian Chukwu dies at...\n",
      "Extracted content for: Former Super Eagles coach Christian Chukwu dies at...\n",
      "\n",
      "Scraping completed. Saved 30 articles\n"
     ]
    }
   ],
   "source": [
    "### scrapes with all columns including content together\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Session directory to maintain cookies between runs\n",
    "SESSION_DIR = os.path.join(os.getcwd(), \"chrome_session\")\n",
    "os.makedirs(SESSION_DIR, exist_ok=True)\n",
    "\n",
    "# File to store checkpoint data\n",
    "CHECKPOINT_FILE = \"scraper_checkpoint.json\"\n",
    "\n",
    "def remove_existing_files():\n",
    "    \"\"\"Remove existing checkpoint and data files to ensure a fresh start\"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        os.remove(CHECKPOINT_FILE)\n",
    "    if os.path.exists('businessday_progress3.csv'):\n",
    "        os.remove('businessday_progress3.csv')\n",
    "    if os.path.exists('businessday_final3.csv'):\n",
    "        os.remove('businessday_final3.csv')\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Set up the undetected Chrome driver with necessary options\"\"\"\n",
    "    options = uc.ChromeOptions()\n",
    "    # Add user data directory to maintain session/cookies\n",
    "    options.add_argument(f\"--user-data-dir={SESSION_DIR}\")\n",
    "    # Make browser less detectable\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-infobars\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    # Use a realistic user agent\n",
    "    options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36\")\n",
    "    # Initialize undetected-chromedriver which helps bypass Cloudflare\n",
    "    driver = uc.Chrome(options=options)\n",
    "    # Set page load timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def random_sleep(min_seconds=2, max_seconds=5):\n",
    "    \"\"\"Sleep for a random amount of time to appear more human-like\"\"\"\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "def human_like_scroll(driver):\n",
    "    \"\"\"Scroll down the page in a human-like manner\"\"\"\n",
    "    total_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    current_position = 0\n",
    "    while current_position < total_height:\n",
    "        scroll_increment = random.randint(100, 800)\n",
    "        current_position += scroll_increment\n",
    "        if current_position > total_height:\n",
    "            current_position = total_height\n",
    "        driver.execute_script(f\"window.scrollTo(0, {current_position});\")\n",
    "        random_sleep(0.5, 1.5)\n",
    "\n",
    "def scrape_page(driver, scraped_data):\n",
    "    \"\"\"\n",
    "    Scrape the current page for articles.\n",
    "    Each article is stored as a dictionary with content initially set to None.\n",
    "    Returns a list of new articles scraped from this page.\n",
    "    \"\"\"\n",
    "    if \"Just a moment\" in driver.title:\n",
    "        print(\"Still on Cloudflare challenge page. Waiting...\")\n",
    "        time.sleep(15)\n",
    "        return []\n",
    "\n",
    "    print(f\"Current page title: {driver.title}\")\n",
    "    human_like_scroll(driver)\n",
    "    new_articles = []\n",
    "    try:\n",
    "        # Wait for the news container to be present\n",
    "        news_container = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.news\"))\n",
    "        )\n",
    "        # Find all news items\n",
    "        news_items = news_container.find_elements(By.XPATH, './/div[@class=\"post-info\"]')\n",
    "        print(f\"Found {len(news_items)} news items\")\n",
    "        if not news_items:\n",
    "            print(\"No news items found on the page.\")\n",
    "            return new_articles\n",
    "\n",
    "        for item in news_items:\n",
    "            try:\n",
    "                title_element = item.find_element(By.TAG_NAME, 'h2').find_element(By.TAG_NAME, 'a')\n",
    "                title = title_element.text\n",
    "                url = title_element.get_attribute(\"href\")\n",
    "                author = item.find_element(By.CLASS_NAME, 'post-author').find_element(By.TAG_NAME, 'a').text\n",
    "                date = item.find_element(By.CLASS_NAME, 'post-date').text\n",
    "                excerpt = item.find_element(By.TAG_NAME, 'p').text\n",
    "\n",
    "                # Avoid duplicates by checking if an article with the same title already exists\n",
    "                if not any(title == article['Title'] for article in scraped_data):\n",
    "                    article_dict = {\n",
    "                        'Title': title,\n",
    "                        'Author': author,\n",
    "                        'Date': date,\n",
    "                        'Excerpt': excerpt,\n",
    "                        'URL': url,\n",
    "                        'Content': None  # Content will be filled in immediately after scraping the page\n",
    "                    }\n",
    "                    scraped_data.append(article_dict)\n",
    "                    new_articles.append(article_dict)\n",
    "                    print(f\"Added article: {title[:50]}...\")\n",
    "                else:\n",
    "                    print(f\"Skipping duplicate article: {title[:50]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing news item: {e}\")\n",
    "                continue\n",
    "        return new_articles\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping page: {e}\")\n",
    "        return new_articles\n",
    "\n",
    "def extract_article_content(driver, article):\n",
    "    \"\"\"\n",
    "    Opens the article URL in a new tab (using Selenium's new_window), extracts its content,\n",
    "    updates the article dictionary, and then closes the tab.\n",
    "    \"\"\"\n",
    "    original_window = driver.current_window_handle\n",
    "    try:\n",
    "        # Open a new tab using Selenium 4's method.\n",
    "        driver.switch_to.new_window('tab')\n",
    "        driver.get(article['URL'])\n",
    "        # Allow the page to load\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            content_element = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, 'post-content'))\n",
    "            )\n",
    "            content = content_element.text\n",
    "            article['Content'] = content.strip()\n",
    "            print(f\"Extracted content for: {article['Title'][:50]}...\")\n",
    "        except Exception as e:\n",
    "            article['Content'] = 'Content not found'\n",
    "            print(f\"Content extraction failed for: {article['Title'][:50]}: {e}\")\n",
    "    except Exception as e:\n",
    "        article['Content'] = 'Content extraction error'\n",
    "        print(f\"Error processing article URL {article['URL']}: {e}\")\n",
    "    finally:\n",
    "        # Close the tab and switch back to the original window\n",
    "        driver.close()\n",
    "        driver.switch_to.window(original_window)\n",
    "\n",
    "def main():\n",
    "    print(\"Starting fresh scrape...\")\n",
    "    # Remove existing files to ensure a fresh start\n",
    "    remove_existing_files()\n",
    "\n",
    "    # Initialize fresh scraping variables\n",
    "    page_count = 1\n",
    "    start_url = 'https://businessday.ng/tag/bdlead/?amp'\n",
    "    scraped_data = []\n",
    "\n",
    "    # Setup the driver\n",
    "    driver = setup_driver()\n",
    "\n",
    "    try:\n",
    "        # Navigate to the starting page\n",
    "        print(f\"Navigating to {start_url}\")\n",
    "        driver.get(start_url)\n",
    "        print(\"Initial page loaded\")\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Set the number of pages you want to process\n",
    "        max_pages = 3\n",
    "        while page_count <= max_pages:\n",
    "            print(f\"\\nProcessing page {page_count}\")\n",
    "            current_url = driver.current_url\n",
    "\n",
    "            # Scrape the page and get new articles\n",
    "            new_articles = scrape_page(driver, scraped_data)\n",
    "            if new_articles:\n",
    "                print(f\"Successfully scraped page {page_count} with {len(new_articles)} new articles\")\n",
    "            else:\n",
    "                print(f\"Failed to scrape page {page_count} or found no new articles\")\n",
    "\n",
    "            # For each new article scraped on this page, immediately extract the full content\n",
    "            for article in new_articles:\n",
    "                print(f\"Downloading content for article: {article['Title'][:50]}...\")\n",
    "                extract_article_content(driver, article)\n",
    "\n",
    "            # Save intermediate progress after each page\n",
    "            pd.DataFrame(scraped_data).to_csv('businessday_progress3.csv', index=False)\n",
    "\n",
    "            # Break if we have reached our target\n",
    "            if page_count >= max_pages:\n",
    "                break\n",
    "\n",
    "            # Navigate to the next page\n",
    "            try:\n",
    "                print(\"Looking for next page button...\")\n",
    "                next_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//a[@class=\"next page-numbers\"]'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_button)\n",
    "                random_sleep(1, 2)\n",
    "                print(\"Clicking next page button...\")\n",
    "                driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                print(\"Waiting for next page to load...\")\n",
    "                random_sleep(8, 12)\n",
    "                page_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error navigating to next page: {e}\")\n",
    "                print(\"Trying alternative method...\")\n",
    "                try:\n",
    "                    if \"page/\" in current_url:\n",
    "                        parts = current_url.split(\"page/\")\n",
    "                        current_page_num = int(parts[1].split(\"/\")[0])\n",
    "                        next_page_num = current_page_num + 1\n",
    "                        next_url = f\"{parts[0]}page/{next_page_num}/\"\n",
    "                    else:\n",
    "                        next_url = f\"{current_url}page/2/\"\n",
    "                    print(f\"Navigating directly to: {next_url}\")\n",
    "                    driver.get(next_url)\n",
    "                    random_sleep(8, 12)\n",
    "                    page_count += 1\n",
    "                except Exception as nav_error:\n",
    "                    print(f\"Failed to navigate to next page: {nav_error}\")\n",
    "                    break\n",
    "\n",
    "        # Save the final results\n",
    "        if scraped_data:\n",
    "            df = pd.DataFrame(scraped_data, columns=['Title', 'Author', 'Date', 'Excerpt', 'URL', 'Content'])\n",
    "            df.to_csv('businessday_final3.csv', index=False)\n",
    "            print(f\"\\nScraping completed. Saved {len(scraped_data)} articles\")\n",
    "        else:\n",
    "            print(\"No data was scraped\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nScraping interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bd39a68-0ffe-437f-84ad-be08e817ae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scraper...\n",
      "Loaded checkpoint: Page 820 from URL: https://businessday.ng/tag/bdlead/page/808/\n",
      "Loaded 3275 existing articles from CSV\n",
      "Navigating to https://businessday.ng/tag/bdlead/page/808/\n",
      "Initial page loaded\n",
      "\n",
      "Processing page 820\n",
      "Still on Cloudflare challenge page. Waiting...\n",
      "Failed to scrape page 820 or found no new articles\n",
      "Checkpoint saved: Page 820 - URL: https://businessday.ng/tag/bdlead/page/808/\n",
      "\n",
      "Scraping completed. Saved 3275 articles\n"
     ]
    }
   ],
   "source": [
    "### Scrapes all columns with content and continues from checkpoint\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Session directory to maintain cookies between runs\n",
    "SESSION_DIR = os.path.join(os.getcwd(), \"chrome_session\")\n",
    "os.makedirs(SESSION_DIR, exist_ok=True)\n",
    "\n",
    "# File to store checkpoint data\n",
    "CHECKPOINT_FILE = \"scraper_checkpoint.json\"\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"Load existing checkpoint if available.\"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        try:\n",
    "            with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "                checkpoint = json.load(f)\n",
    "            print(f\"Loaded checkpoint: Page {checkpoint.get('page_count')} from URL: {checkpoint.get('current_url')}\")\n",
    "            return checkpoint\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "    return None\n",
    "\n",
    "def save_checkpoint(page_count, current_url):\n",
    "    \"\"\"Save current progress to a checkpoint file.\"\"\"\n",
    "    checkpoint = {\n",
    "        \"page_count\": page_count,\n",
    "        \"current_url\": current_url,\n",
    "        \"last_update\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    try:\n",
    "        with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "            json.dump(checkpoint, f)\n",
    "        print(f\"Checkpoint saved: Page {page_count} - URL: {current_url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving checkpoint: {e}\")\n",
    "\n",
    "def load_existing_data():\n",
    "    \"\"\"Load previously scraped articles from CSV if available.\"\"\"\n",
    "    if os.path.exists('businessday_prog12345.csv'):\n",
    "        try:\n",
    "            df = pd.read_csv('businessday_prog12345.csv')\n",
    "            print(f\"Loaded {len(df)} existing articles from CSV\")\n",
    "            return df.to_dict('records')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading existing data: {e}\")\n",
    "    return []\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Set up the undetected Chrome driver with necessary options.\"\"\"\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(f\"--user-data-dir={SESSION_DIR}\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-infobars\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36\")\n",
    "    driver = uc.Chrome(options=options)\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def random_sleep(min_seconds=2, max_seconds=5):\n",
    "    \"\"\"Sleep for a random amount of time to appear more human-like.\"\"\"\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "def human_like_scroll(driver):\n",
    "    \"\"\"Scroll down the page in a human-like manner.\"\"\"\n",
    "    total_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    current_position = 0\n",
    "    while current_position < total_height:\n",
    "        scroll_increment = random.randint(100, 800)\n",
    "        current_position += scroll_increment\n",
    "        if current_position > total_height:\n",
    "            current_position = total_height\n",
    "        driver.execute_script(f\"window.scrollTo(0, {current_position});\")\n",
    "        random_sleep(0.5, 1.5)\n",
    "\n",
    "def scrape_page(driver, scraped_data):\n",
    "    \"\"\"\n",
    "    Scrape the current page for articles.\n",
    "    Each article is stored as a dictionary with content initially set to None.\n",
    "    Returns a list of new articles scraped from this page.\n",
    "    \"\"\"\n",
    "    if \"Just a moment\" in driver.title:\n",
    "        print(\"Still on Cloudflare challenge page. Waiting...\")\n",
    "        time.sleep(15)\n",
    "        return []\n",
    "\n",
    "    print(f\"Current page title: {driver.title}\")\n",
    "    human_like_scroll(driver)\n",
    "    new_articles = []\n",
    "    try:\n",
    "        news_container = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.news\"))\n",
    "        )\n",
    "        news_items = news_container.find_elements(By.XPATH, './/div[@class=\"post-info\"]')\n",
    "        print(f\"Found {len(news_items)} news items\")\n",
    "        if not news_items:\n",
    "            print(\"No news items found on the page.\")\n",
    "            return new_articles\n",
    "\n",
    "        for item in news_items:\n",
    "            try:\n",
    "                title_element = item.find_element(By.TAG_NAME, 'h2').find_element(By.TAG_NAME, 'a')\n",
    "                title = title_element.text\n",
    "                url = title_element.get_attribute(\"href\")\n",
    "                author = item.find_element(By.CLASS_NAME, 'post-author').find_element(By.TAG_NAME, 'a').text\n",
    "                date = item.find_element(By.CLASS_NAME, 'post-date').text\n",
    "                excerpt = item.find_element(By.TAG_NAME, 'p').text\n",
    "\n",
    "                if not any(title == article['Title'] for article in scraped_data):\n",
    "                    article_dict = {\n",
    "                        'Title': title,\n",
    "                        'Author': author,\n",
    "                        'Date': date,\n",
    "                        'Excerpt': excerpt,\n",
    "                        'URL': url,\n",
    "                        'Content': None\n",
    "                    }\n",
    "                    scraped_data.append(article_dict)\n",
    "                    new_articles.append(article_dict)\n",
    "                    print(f\"Added article: {title[:50]}...\")\n",
    "                else:\n",
    "                    print(f\"Skipping duplicate article: {title[:50]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing news item: {e}\")\n",
    "                continue\n",
    "        return new_articles\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping page: {e}\")\n",
    "        return new_articles\n",
    "\n",
    "def extract_article_content(driver, article):\n",
    "    \"\"\"\n",
    "    Open the article URL in a new tab (using Selenium's new window method),\n",
    "    extract its content, update the article dict, and then close the tab.\n",
    "    \"\"\"\n",
    "    original_window = driver.current_window_handle\n",
    "    try:\n",
    "        driver.switch_to.new_window('tab')\n",
    "        driver.get(article['URL'])\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            content_element = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, 'post-content'))\n",
    "            )\n",
    "            content = content_element.text\n",
    "            article['Content'] = content.strip()\n",
    "            print(f\"Extracted content for: {article['Title'][:50]}...\")\n",
    "        except Exception as e:\n",
    "            article['Content'] = 'Content not found'\n",
    "            print(f\"Content extraction failed for: {article['Title'][:50]}: {e}\")\n",
    "    except Exception as e:\n",
    "        article['Content'] = 'Content extraction error'\n",
    "        print(f\"Error processing article URL {article['URL']}: {e}\")\n",
    "    finally:\n",
    "        driver.close()\n",
    "        driver.switch_to.window(original_window)\n",
    "\n",
    "def main():\n",
    "    print(\"Starting scraper...\")\n",
    "\n",
    "    # Load checkpoint and existing articles if available.\n",
    "    checkpoint = load_checkpoint()\n",
    "    scraped_data = load_existing_data()\n",
    "\n",
    "    # Determine starting page and URL.\n",
    "    if checkpoint:\n",
    "        page_count = checkpoint.get(\"page_count\", 1)\n",
    "        start_url = checkpoint.get(\"current_url\", \"https://businessday.ng/tag/bdlead/?amp\")\n",
    "    else:\n",
    "        page_count = 1\n",
    "        start_url = 'https://businessday.ng/tag/bdlead/?amp'\n",
    "        print(\"No checkpoint found. Starting from page 1.\")\n",
    "\n",
    "    # Setup the driver.\n",
    "    driver = setup_driver()\n",
    "\n",
    "    try:\n",
    "        print(f\"Navigating to {start_url}\")\n",
    "        driver.get(start_url)\n",
    "        print(\"Initial page loaded\")\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Set the maximum pages to process (adjust as needed).\n",
    "        max_pages = 820\n",
    "        while page_count <= max_pages:\n",
    "            print(f\"\\nProcessing page {page_count}\")\n",
    "            current_url = driver.current_url\n",
    "\n",
    "            new_articles = scrape_page(driver, scraped_data)\n",
    "            if new_articles:\n",
    "                print(f\"Successfully scraped page {page_count} with {len(new_articles)} new articles\")\n",
    "            else:\n",
    "                print(f\"Failed to scrape page {page_count} or found no new articles\")\n",
    "\n",
    "            # Extract content for new articles immediately.\n",
    "            for article in new_articles:\n",
    "                print(f\"Downloading content for article: {article['Title'][:50]}...\")\n",
    "                extract_article_content(driver, article)\n",
    "\n",
    "            # Save intermediate progress.\n",
    "            pd.DataFrame(scraped_data).to_csv('businessday_progress301.csv', index=False)\n",
    "            save_checkpoint(page_count, current_url)\n",
    "\n",
    "            if page_count >= max_pages:\n",
    "                break\n",
    "\n",
    "            # Navigate to the next page.\n",
    "            try:\n",
    "                print(\"Looking for next page button...\")\n",
    "                next_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//a[@class=\"next page-numbers\"]'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_button)\n",
    "                random_sleep(1, 2)\n",
    "                print(\"Clicking next page button...\")\n",
    "                driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                print(\"Waiting for next page to load...\")\n",
    "                random_sleep(8, 12)\n",
    "                page_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error navigating to next page: {e}\")\n",
    "                print(\"Trying alternative method...\")\n",
    "                try:\n",
    "                    if \"page/\" in current_url:\n",
    "                        parts = current_url.split(\"page/\")\n",
    "                        current_page_num = int(parts[1].split(\"/\")[0])\n",
    "                        next_page_num = current_page_num + 1\n",
    "                        next_url = f\"{parts[0]}page/{next_page_num}/\"\n",
    "                    else:\n",
    "                        next_url = f\"{current_url}page/2/\"\n",
    "                    print(f\"Navigating directly to: {next_url}\")\n",
    "                    driver.get(next_url)\n",
    "                    random_sleep(8, 12)\n",
    "                    page_count += 1\n",
    "                except Exception as nav_error:\n",
    "                    print(f\"Failed to navigate to next page: {nav_error}\")\n",
    "                    break\n",
    "\n",
    "        # Save final results.\n",
    "        if scraped_data:\n",
    "            df = pd.DataFrame(scraped_data, columns=['Title', 'Author', 'Date', 'Excerpt', 'URL', 'Content'])\n",
    "            df.to_csv('businessday_final301.csv', index=False)\n",
    "            print(f\"\\nScraping completed. Saved {len(scraped_data)} articles\")\n",
    "        else:\n",
    "            print(\"No data was scraped\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nScraping interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "766dda0d-79fa-40bd-abd4-79af1da7db81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scraper...\n",
      "Loaded checkpoint: Page 936 from URL: https://businessday.ng/tag/bdlead/page/924/\n",
      "Loaded 3275 existing articles from CSV\n",
      "Resuming from page 936 at https://businessday.ng/tag/bdlead/page/924/\n",
      "Page loaded. Waiting a moment for any challenges…\n",
      "\n",
      "Processing page 936\n",
      "Current page title: BDlead Archives - Page 924 of 1643 - Businessday NG\n",
      "Found 0 news items\n",
      "No news items found on the page.\n",
      "  → No new articles found on this page\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'businessday_progress3.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 260\u001b[0m\n\u001b[0;32m    257\u001b[0m         driver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 260\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[32], line 210\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    207\u001b[0m     extract_article_content(driver, art)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# Save progress and checkpoint\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(scraped_data)\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbusinessday_progress3.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    211\u001b[0m save_checkpoint(page_count, driver\u001b[38;5;241m.\u001b[39mcurrent_url)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# Try clicking “Next”…\u001b[39;00m\n",
      "File \u001b[1;32m~\\anacodaa\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anacodaa\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3968\u001b[0m     path_or_buf,\n\u001b[0;32m   3969\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3970\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3971\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3972\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3973\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3974\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3975\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3976\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3977\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3978\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3979\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3980\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3981\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3982\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3983\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3984\u001b[0m )\n",
      "File \u001b[1;32m~\\anacodaa\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\anacodaa\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    254\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    255\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    256\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    257\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\anacodaa\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'businessday_progress3.csv'"
     ]
    }
   ],
   "source": [
    "### Scrapes all columns with content and continues from checkpoint\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Session directory to maintain cookies between runs\n",
    "SESSION_DIR = os.path.join(os.getcwd(), \"chrome_session\")\n",
    "os.makedirs(SESSION_DIR, exist_ok=True)\n",
    "\n",
    "# File to store checkpoint data\n",
    "CHECKPOINT_FILE = \"scraper_checkpoint.json\"\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"Load existing checkpoint if available.\"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        try:\n",
    "            with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "                checkpoint = json.load(f)\n",
    "            print(f\"Loaded checkpoint: Page {checkpoint.get('page_count')} from URL: {checkpoint.get('current_url')}\")\n",
    "            return checkpoint\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "    return None\n",
    "\n",
    "def save_checkpoint(page_count, current_url):\n",
    "    \"\"\"Save current progress to a checkpoint file.\"\"\"\n",
    "    checkpoint = {\n",
    "        \"page_count\": page_count,\n",
    "        \"current_url\": current_url,\n",
    "        \"last_update\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    try:\n",
    "        with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "            json.dump(checkpoint, f)\n",
    "        print(f\"Checkpoint saved: Page {page_count} - URL: {current_url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving checkpoint: {e}\")\n",
    "\n",
    "def load_existing_data():\n",
    "    \"\"\"Load previously scraped articles from CSV if available.\"\"\"\n",
    "    if os.path.exists('businessday_prog12345.csv'):\n",
    "        try:\n",
    "            df = pd.read_csv('businessday_prog12345.csv')\n",
    "            print(f\"Loaded {len(df)} existing articles from CSV\")\n",
    "            return df.to_dict('records')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading existing data: {e}\")\n",
    "    return []\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Set up the undetected Chrome driver with necessary options.\"\"\"\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(f\"--user-data-dir={SESSION_DIR}\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-infobars\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36\")\n",
    "    driver = uc.Chrome(options=options)\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def random_sleep(min_seconds=2, max_seconds=5):\n",
    "    \"\"\"Sleep for a random amount of time to appear more human-like.\"\"\"\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "def human_like_scroll(driver):\n",
    "    \"\"\"Scroll down the page in a human-like manner.\"\"\"\n",
    "    total_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    current_position = 0\n",
    "    while current_position < total_height:\n",
    "        scroll_increment = random.randint(100, 800)\n",
    "        current_position += scroll_increment\n",
    "        if current_position > total_height:\n",
    "            current_position = total_height\n",
    "        driver.execute_script(f\"window.scrollTo(0, {current_position});\")\n",
    "        random_sleep(0.5, 1.5)\n",
    "\n",
    "def scrape_page(driver, scraped_data):\n",
    "    \"\"\"\n",
    "    Scrape the current page for articles.\n",
    "    Each article is stored as a dictionary with content initially set to None.\n",
    "    Returns a list of new articles scraped from this page.\n",
    "    \"\"\"\n",
    "    if \"Just a moment\" in driver.title:\n",
    "        print(\"Still on Cloudflare challenge page. Waiting...\")\n",
    "        time.sleep(15)\n",
    "        return []\n",
    "\n",
    "    print(f\"Current page title: {driver.title}\")\n",
    "    human_like_scroll(driver)\n",
    "    new_articles = []\n",
    "    try:\n",
    "        news_container = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.news\"))\n",
    "        )\n",
    "        news_items = news_container.find_elements(By.XPATH, './/div[@class=\"post-info\"]')\n",
    "        print(f\"Found {len(news_items)} news items\")\n",
    "        if not news_items:\n",
    "            print(\"No news items found on the page.\")\n",
    "            return new_articles\n",
    "\n",
    "        for item in news_items:\n",
    "            try:\n",
    "                title_element = item.find_element(By.TAG_NAME, 'h2').find_element(By.TAG_NAME, 'a')\n",
    "                title = title_element.text\n",
    "                url = title_element.get_attribute(\"href\")\n",
    "                author = item.find_element(By.CLASS_NAME, 'post-author').find_element(By.TAG_NAME, 'a').text\n",
    "                date = item.find_element(By.CLASS_NAME, 'post-date').text\n",
    "                excerpt = item.find_element(By.TAG_NAME, 'p').text\n",
    "\n",
    "                if not any(title == article['Title'] for article in scraped_data):\n",
    "                    article_dict = {\n",
    "                        'Title': title,\n",
    "                        'Author': author,\n",
    "                        'Date': date,\n",
    "                        'Excerpt': excerpt,\n",
    "                        'URL': url,\n",
    "                        'Content': None\n",
    "                    }\n",
    "                    scraped_data.append(article_dict)\n",
    "                    new_articles.append(article_dict)\n",
    "                    print(f\"Added article: {title[:50]}...\")\n",
    "                else:\n",
    "                    print(f\"Skipping duplicate article: {title[:50]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing news item: {e}\")\n",
    "                continue\n",
    "        return new_articles\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping page: {e}\")\n",
    "        return new_articles\n",
    "\n",
    "def extract_article_content(driver, article):\n",
    "    \"\"\"\n",
    "    Open the article URL in a new tab (using Selenium's new window method),\n",
    "    extract its content, update the article dict, and then close the tab.\n",
    "    \"\"\"\n",
    "    original_window = driver.current_window_handle\n",
    "    try:\n",
    "        driver.switch_to.new_window('tab')\n",
    "        driver.get(article['URL'])\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            content_element = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, 'post-content'))\n",
    "            )\n",
    "            content = content_element.text\n",
    "            article['Content'] = content.strip()\n",
    "            print(f\"Extracted content for: {article['Title'][:50]}...\")\n",
    "        except Exception as e:\n",
    "            article['Content'] = 'Content not found'\n",
    "            print(f\"Content extraction failed for: {article['Title'][:50]}: {e}\")\n",
    "    except Exception as e:\n",
    "        article['Content'] = 'Content extraction error'\n",
    "        print(f\"Error processing article URL {article['URL']}: {e}\")\n",
    "    finally:\n",
    "        driver.close()\n",
    "        driver.switch_to.window(original_window)\n",
    "\n",
    "def main():\n",
    "    print(\"Starting scraper...\")\n",
    "\n",
    "    # Load checkpoint and existing articles if available.\n",
    "    checkpoint = load_checkpoint()\n",
    "    scraped_data = load_existing_data()\n",
    "\n",
    "    # Determine starting page and URL.\n",
    "    if checkpoint:\n",
    "        page_count = checkpoint.get(\"page_count\", 1)\n",
    "        current_url = checkpoint.get(\"current_url\", \"https://businessday.ng/tag/bdlead/?amp\")\n",
    "        print(f\"Resuming from page {page_count} at {current_url}\")\n",
    "    else:\n",
    "        page_count = 1\n",
    "        current_url = 'https://businessday.ng/tag/bdlead/?amp'\n",
    "        print(\"No checkpoint found. Starting from page 1.\")\n",
    "    driver = setup_driver()\n",
    "\n",
    "    try:\n",
    "        driver.get(current_url)\n",
    "        print(\"Page loaded. Waiting a moment for any challenges…\")\n",
    "        time.sleep(5)\n",
    "\n",
    "        while True:\n",
    "            print(f\"\\nProcessing page {page_count}\")\n",
    "            new_articles = scrape_page(driver, scraped_data)\n",
    "            if new_articles:\n",
    "                print(f\"  → {len(new_articles)} new articles found\")\n",
    "            else:\n",
    "                print(\"  → No new articles found on this page\")\n",
    "\n",
    "            # Immediately pull full content for each new article\n",
    "            for art in new_articles:\n",
    "                print(f\"    * Downloading content for: {art['Title'][:40]}…\")\n",
    "                extract_article_content(driver, art)\n",
    "\n",
    "            # Save progress and checkpoint\n",
    "            pd.DataFrame(scraped_data).to_csv('businessday_progress3.csv', index=False)\n",
    "            save_checkpoint(page_count, driver.current_url)\n",
    "\n",
    "            # Try clicking “Next”…\n",
    "            try:\n",
    "                next_btn = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//a[@class=\"next page-numbers\"]'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_btn)\n",
    "                random_sleep(1, 2)\n",
    "                next_btn.click()\n",
    "                random_sleep(8, 12)\n",
    "                page_count += 1\n",
    "                continue\n",
    "            except Exception:\n",
    "                # Fallback: try constructing the next-page URL\n",
    "                print(\"Next button not found—trying direct URL method…\")\n",
    "                try:\n",
    "                    url = driver.current_url\n",
    "                    if \"page/\" in url:\n",
    "                        base, num = url.rsplit(\"page/\", 1)\n",
    "                        next_page = int(num.split(\"/\")[0]) + 1\n",
    "                        next_url = f\"{base}page/{next_page}/\"\n",
    "                    else:\n",
    "                        next_url = url.rstrip(\"/\") + \"/page/2/\"\n",
    "                    print(f\"Navigating directly to {next_url}\")\n",
    "                    driver.get(next_url)\n",
    "                    random_sleep(8,12)\n",
    "                    page_count += 1\n",
    "                    continue\n",
    "                except Exception:\n",
    "                    print(\"Could not navigate to the next page. Assuming end of pages.\")\n",
    "                    break\n",
    "\n",
    "        # final save\n",
    "        if scraped_data:\n",
    "            df = pd.DataFrame(scraped_data,\n",
    "                              columns=['Title','Author','Date','Excerpt','URL','Content'])\n",
    "            df.to_csv('businessday_final3.csv', index=False)\n",
    "            print(f\"\\nScraping completed. Saved {len(scraped_data)} articles.\")\n",
    "        else:\n",
    "            print(\"No data was scraped.\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted by user—saving progress!\")\n",
    "        pd.DataFrame(scraped_data).to_csv('businessday_progress3.csv', index=False)\n",
    "        save_checkpoint(page_count, driver.current_url)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1520edb6-4239-4fa3-8199-c67370703b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'C:\\\\Users\\\\USER\\\\webs_scraper.ipynb': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python webs_scraper.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4fe94823-6e30-4e56-b972-0ebf4fb2b77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "python: can't open file 'C:\\\\Users\\\\USER\\\\webs_scraper.ipynb': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm scraper_checkpoint.json businessday_progress3.csv\n",
    "!python webs_scraper.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a30dfb6c-ac01-46e1-802a-ac926a7fe00a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3305369356.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[70], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    jupyter notebook\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b0d580-299e-4e88-bbc8-c1b3b53e687c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
